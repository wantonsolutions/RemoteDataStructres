\section{Background}
\label{sec:background}

\subsection{RDMA}

\todo{this section needs to be crunched down}

\begin{figure*}[t]
    \centering
    \begin{subfigure}{0.3\linewidth}
        \includegraphics[width=0.99\linewidth]{fig/rdma_latency.pdf}
        % \label{fig:rdma_latency}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}{0.3\linewidth}
        \includegraphics[width=0.99\linewidth]{fig/rdma_concur.pdf}
        % \label{fig:optimistic_failures}
        % \caption{}
    \end{subfigure}
    \begin{subfigure}{0.3\linewidth}
        \includegraphics[width=0.99\linewidth]{fig/rdma_cas_throughput.pdf}
        % \label{fig:optimistic_failures}
        % \caption{}
    \end{subfigure}
    \vspace{-1em}
    \caption{
    \textbf{(a)} CX5 RDMA latency vs message size~\cite{rdma-latency}
    \textbf{(b)} RDMA operation scalability
    \textbf{(c)} Compare and swap performance. Device memory vs main memory.
    }
    \label{fig:rdma-benchmarks}
\end{figure*}

RDMA is an enabling technology for memory disaggregation. It
allows client machines to directly access the memory of a
remote server with operations like read, write and atomic
update, without the involvement of a remote CPU (save for
initialization).  
%%
In the past decade the bandwidth
capabilities of RDMA NICs have increased disproportionately
to their latency improvement. The bandwidth difference
between CX3 and CX7 NICs is 10$\times$ and yet intra-rack
round trips of small RDMA packets has only shrunk by around
1.5$\times$.  Figure~\ref{fig:rdma-benchmarks}(a) shows the
tradeoff between NIC-to-NIC round trips times on CX5 RDMA
NICs for write operations. All packets up to 128 bytes have
comparable round trip latencies, and packet sizes must grow
to above 1K before the latency cost of a large packet
exceeds the cost of two round trips for smaller packets.
\textbf{If there is surplus bandwidth a single large message
can have much lower latency than two dependent smaller
messages}

RDMA verbs do not all have the same performance. Atomic
operations such as compare-and-swap (CAS) and fetch-and-add
(FAA) both have bottlenecks much lower than reads and
writes~\cite{design-guidelines,sherman}.
Figure~\ref{fig:rdma-benchmarks}(b) shows the scalability of
these operations on CX5 NICs for 64 bit operations.
%%
Atomic operations bottleneck due to PCIe round trips. 
Each operation issued from NIC to host memory requires a
PCIe round trip. Data dependent operations must queue until
the atomic operation completes.
%%
Vendors have recently provided a small region (256KB)
of device mapped memory which avoids the round trip on
atomic operations~\cite{device-memory}.
Figure~\ref{fig:rdma-benchmarks}(c) shows the relative
performance of CAS operation on device memory vs host
memory. Lock request on a single address are 3x higher
throughput, and independent lock requests scale at nearly
the same rate as read and write requests.

Additionally Mellanox provides non-standard verbs such as
64-bit masked compare-and-swap which sets individual bits in
a 64 bit word independently of unmasked
bits~\cite{rdma-masked-cas} This is useful for setting
multiple independent locks, assuming locks are close
together, as the state of all locks need not be known.


\subsection{RDMA Key-Value Stores}

%% %Describe existing key value stores relation to rdma and
%serialization %
Many non-disaggregated key-value stores have used RDMA to
accelerate their
performance~\cite{farm,memc3,erpc,herd,faast,mica,pilaf,cell,storm}.
%%
These systems strike a careful balance between directly
accessing memory with client side RDMA and serializing
requests with a server side CPU.
%%
Cuckoo and Hopscotch hashes have flourished in this space
because clients can locally calculate the location of keys
and perform lockless $O(1)$ reads with
RDMA~\cite{hopscotch,farm,pilaf,cuckoo}.
%%
Disaggregated key-value in contrast assume that a memory
server cannot provide serialization and orchestrate their
writes solely using
clients~\cite{rolex,fusee,clover,sherman,ford,race}. With
the exception of Sherman~\cite{sherman} these systems use
systems commit writes optimistically using 64 bit RDMA CAS. 
%%
Opportunistic writes have the advantage that updates are
atomically visible, no critical sections exist, and client
failures do not leave the table in an inconsistent state.
Unfortunately CAS based opportunistic writes perform poorly
under contention leading to high tail latencies
~\cite{clover}. Additionally RDMA CAS does not scale
well~\cite{design-guidelines}(Figure~\ref{fig:rdma-benchmarks}(b)),
and their small word width and lack of multi-CAS support
constrain the size and types of updates they can perform.

CAS width in particular effects system design because
key-value pairs can rarely fit into 64 bits, indexes updated
with CAS must reference keys values indirectly with a
pointer. At minimum resolving a remote pointer requires an
additional round trip for every read~\cite{race,clover}.
%%
As we will show in the following section data structures
like cuckoo and hopscotch hashes are difficult implement with
optimistic CAS updates because they require multiple updates
to execute atomically.


\subsection{Cuckoo Hashing} 
\label{sec:cuckoo-back}
Cuckoo hashing uses two independent hash functions to assign
keys a primary and secondary table location. When a key is
inserted if its primary location is occupied the existing
key is evicted to its alternative location. If the eviction
causes another collision the process iterates until an open
location is found. The path of evictions is known as
a~\textit{cuckoo path}. Cuckoo hashes use associative rows
to improve their maximum fill factors. In associative hashes
multiple clients can be chosen as eviction candidates and
breadth-first-search (BFS) has been shown to minimize both
cuckoo path length and critical section time~\cite{memc3,
cuckoo-improvements}.  While insertions require large
critical sections to perform search and execute updates
along the cuckoo path reads are executed in constant time by
reading both of a keys buckets~\cite{pilaf}.

%%now I want to explain why cuckoo hashing and disaggregation don't work well together.

Lockless $O(1)$ reads make Cuckoo hashing a desireable
candidate for a disaggregated index. However, long
unpredictable cuckoo paths and RDMA CAS limitations make
performing insertions without locks difficult in the
disaggregated setting. We designed an RDMA based cuckoo hash
to illustrate the difficulties of performing opportunistic
insertions. On inserts this system makes a sequence of reads
to calculate a valid cuckoo path and then itteratitivly
issues CAS operations to swap value along the path. If any
value on the path is concurrently modified by another client
the insertions will fail and must restart.
Figure~\ref{fig:cuckoo-problems}(a) shows how the failure
rate of insertions filling a table from 80-90\% full.

\textbf{Hopscotch Hashing} also enables fast reads by
localizing entries to a bounded range of addresses and we
considered using it as our core data structure. We believe
that although hopscotch hashing can likely be disaggregated
efficiently it is less straightforward than cuckoo hashing
for the following reasons:
%%
First, cuckoo insertions update one location per moved key while
hopscotch requires two. Hopscotch hashes keep a bitmask of
collisions per bucket which must also be updated when an
entry is relocated. Reno, a heavily one-sided RDMA hopscotch
hash, uses one sided atomics to sloppily update the bitmask
but requires a server side CPU to fix the bitmasks whenever
concurrent inserts execute~\cite{reno}. Both Farm and Reno
avoid ever executing long hopscotch chains because of their
execution time and complexity~\cite{farm,reno}.
%%
Second, an entries location in a cuckoo hash is easier to
predict than in hopscotch hashing which simplifies locking.
Cuckoo keys inhabit exactly one of two locations so locks
are predictable, conversely hopscotch locations occur on a
range so a locking strategy must lock the entire range
conservatively. Because each lock acquisition is expensive
this increases the fundamental difficulty of disaggregation
a hopscotch hash.