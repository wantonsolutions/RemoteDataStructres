\section{Problems}
\label{sec:problems}

Far memory data structure design is difficult because of
its high access latency. Operations which require
synchronization or blocking are amplified by the latency.
As a result, computations that would be considered prohibitively expensive when accessing local DRAM may make sense when accessing far
memory. For example, pushing complexity to the client is
often beneficial as it reduces load on the server. Examples
include caching remote data to improve optimistic
operations, making writes associative and reads large enough
to capture entire associative buckets, and adding
computation to the client to reduce contention. Here we
detail the problems and challenges of using traditional
cuckoo hashing on far memory.


\textbf{Caching:} Modifying a remote data structure is most efficient when 
clients have an up-to-date view of the remote state, i.e., synchronized caches. The cache can either be
accumulated per operation and be discarded or persist across
operations.  Accumulating a cache per operation is slow,
clients must aquire locks, read, then release potentially
many times to complete an operation if the locks are fine
grained. Alternatively clients can persist a cache across
requests in the hope that it will be valid for future
operations.  Clover (a remote memory key value store) caches
pointers to values on clients to enable fast reads when
values are looked up multiple times~\cite{clover}.
Optimistic caching threads a fine line as issuing optimistic
operations which commonly fail may be worse than acquiring
the correct locks.  An ideal caching strategy would enable
clients to succeed in their operations frequently while not
requiring much overhead to maintain. 


%cuckoo hashing optimistic vs locks
\textbf{Critical Sections:} Consider executing an insert
into a concurrent cuckoo hash stored in remote memory. A
client with a cached index may have little or very stale
information about the state of the hash table. To insert the
client must gather information by reading buckets to compute
a cuckoo path. With concurrent clients this leads to a
chicken and egg problem when acquiring locks vs making
reads.

%% optimistic inserts
A client can perform inserts opportunistically by executing
lockless read to learn about the hash table, calculating a
cuckoo path, and executing a sequence of dependent CAS
operations for each step in the path. This approach is
scalable as its critical section is only the length of a CAS
instruction and is only limited by RDMA atomic operation
throughput~\cite{design-guidelines}. However, Paths can
become invalid as other clients running concurrent inserts
invalidate the paths. Figure~\ref{fig:cuckoo-problems}(a) shows the
path insertion failure rate as the number of concurrent
clients grows. This approach minimizes round trips as
dependent CAS operations can be batched thanks to in order
delivery provided by RDMA reliable connections.
Unfortunately failed inserts require additional round trips
to both fix the state of the table, and retry the
insert.\sg{Further - Issuing CAS as a batch leads to complex
path failure cases such a single element in the path failing
while others further down the path succeed. Assesing and
fixing such insertion failures without locks is very hard.}

% lock based inserts
Alternatively to get synchronized information the client can
lock the table, then issue reads. However acquiring locks
without knowledge of the table is hard. A global lock
ensures that all reads are synchronized, but bottlenecks
hash table throughput. Alternatively per-bucket locks
enable high throughput but calculating which buckets to lock
requires knowledge of the table. A long cuckoo path may
require locking many buckets and many round trips to gather
information about the hash table.
%%
An ideal protocol would enable clients to perform inserts
without impeding insertions in other portions of the hash
table, while requiring the fewest round trips to construct
and execute the cuckoo path.

% First, acquiring a lock means a round trip. If the table has
% a single lock, then a client is guaranteed to be able to
% gather all the locks it requires in a single round trip.
% However a single lock does not scale as only a single writer
% can write at a time. This matter is made worse by the fact
% that the critical section of the lock is larger in remote
% memory. Breaking the table up into subtables each with it's
% own lock has it's own problems. An insertion with a long
% path will potentially need to acquire many locks. Each of
% which requires a round trip. Therefore using fine grained
% locking increases the tables scalability but increases it's
% base case insertion time.

\textbf{Read Optimization:} Most data center workloads are
read heavy, therefore read operations should be the most
highly optimized~\cite{datacenter-workloads,facebook-memcached}.
Ideally we would
be able to ensure that reads complete in a single round
trip.
Prior
approaches such as RACE require two RDMA round trips per
read. The first is a hash index lookup, the second round
trip reads the actual key-value block. RACE must perform two
round trips because entries in the hash index are limited to
64 bits (CAS width). This is commonly not enough to store
both key and value so RACE can not inline both keys and
values in the index structure. Clover~\cite{clover} enables
single round trips reads. However under contention Clovers
reads require pointer chasing which is known to be expensive
due to each pointer resolution requiring a round
trip~\cite{clio,clover,pointer-chaising}. 

\textbf{Duplicate Keys:} Clients may issue concurrent
inserts for the same key, given that keys may occupy
multiple location detecting and dealing with duplicate keys
is tricky while maintaining high performance. RACE requires
an extra round trip after each insert to check if a
duplicate key was inserted simultaneously~\cite{race}. An
ideal algorithm would prevent key duplication without
requiring additional overheads.