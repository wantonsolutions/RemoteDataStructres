
\section{Background}
\label{sec:background}

Existing disaggregated key/value stores build upon the extensive literature of
%are the highest performance technology for sharing disaggregated
RDMA-based key/value stores. In this section we overview that lineage and discuss the algorithmic
distinctions between the two. Our hash table design draws on both cuckoo and hopscotch hashing. We
provide a brief overview of both approaches and related literature.


% Key/value stores have long used RDMA to accelerate
% performance~\cite{farm,memc3,erpc,herd,faast,mica,pilaf,cell,storm}.  These systems strike a careful
% balance between directly accessing remote memory with one-sided RDMA requests and serializing
% mutating operations at a server-side CPU.  Cuckoo~\cite{cuckoo} and hopscotch~\cite{hopscotch}
% hashing are popular choices in this space because clients can calculate the location of keys without
% needing to consult the server and potentially perform lock-less reads with
% RDMA~\cite{farm,pilaf,cuckoo}.

\subsection{Disaggregated Key/Value Stores}

The challenge to adapting existing RDMA-based KVS designs to a fully
disaggregated setting is the need to rely exclusively on one-sided
operations due to the lack of a server-side CPU.  Precursor RDMA KVS
systems achieve performance by striking a delicate balance between
efficient one-sided RDMA operations and CPU-serialized two-sided
operations~\cite{farm,herd,pilaf,cell,storm}, where the later are used
only judiciously, but are key to correctness.
%Disaggregated
%KVS must only use one-sided operations even when performing complex
%operations like resolving a collision while inserting.

In general, the performance of any KVS on a mixed read/write workload
hinges on its serialization performance.  Fast consistent writes with
one-sided RDMA are hard because RDMA atomic operations operate on
small (8-byte) regions of memory and the cost of round trips is
high. This constraint has lead to a divide in the design of
disaggregated KVS systems between those that support range queries over
ordered keys and those that do not. Systems with ordered keys use
locks to guard their complex updates, while those
with unordered keys employ lock-free optimistic approaches by
constraining updates to atomic 8-byte writes. These latter designs
typically use hash indexes with 8-byte entries that point to
uncontested extent regions where the corresponding values are stored.
Commonly the index entries use 48 of the 64 bits as a pointer and the
remaining 16 bits for a digest of the key and the size of the value.
This approach requires additional round trips to check for a key's
presence and limits the size of values that can be
stored~\cite{fusee,race}.

On the other hand, KVS systems that support ordered keys use
locks to guard the complex operations required to update B-Trees or
Radix Trees.  In exchange for expensive update operations, locks allow
%The advantage of locks is that
these systems to support inlined values and, thus, faster small-value
reads.  Performance is gated, however, by lock granularity and hold
times. At the time of writing all lock-based KVS systems assume that
clients are grouped into co-located servers which can locally
coordinate lock accesses and batch their writes~\cite{smart,sherman},
an assumption that optimistic
KVS systems do not make~\cite{ditto,fusee,clover,race}. We show that it is possible to
realize the performance gain of lock-guarded inlined operations
without requiring client co-location if locks are sufficiently fine
grained and hold times limited.
%critical sections are kept to 1-RTT and locks are as fine grained as possible.

\subsection{RDMA and Network Performance}

Historically, network bandwidth has limited KVS throughput more than
operation rate. For instance, while a 40-Gbps ConnectX-3 NIC can
process 75 million packets per second, line rate restricts it to
5~MOPS when reading 1-KB objects. As commodity datacenter link rates
increase from 100 Gbps to 400 Gbps and beyond, however, latency and
contention---rather than raw network bandwidth---become the primary
scalability bottlenecks. To this end our design choices seek to reduce
round trips at the cost of slight bandwidth amplification
and we focus our evaluation on workloads with small key/value pairs
whose performance is not limited by line rate on our 100-Gbps ConnectX-5 testbed
hardware.

Concretely, Figure~\ref{fig:rdma-benchmarks}(a) shows the growth in
round trip time for RDMA reads of increasing size. On our hardware,
the round trip time for a single 1-KB read is lower than the total
latency of issuing two dependent reads of just a few bytes each.  This
suggests that performance can be gained if a single large packet can
complete the work of two smaller, but dependent messages.  
%%
Indeed, our
evaluation (see Figure~\ref{fig:microbenchmarks}(d)) shows that even on our
100-Gbps testbed inlining values and servicing lookups with a single
large read provides a 4--37\% throughput increase compared to an
extent-based approach for the same value size.


%% Figure~\ref{fig:rdma-benchmarks}(b) shows the performance
%% overheads of using an extent based design on small key-value
%% pairs. Inlined values are read directly from the index while all other
%% values are read from indirect storage. Here client caching is turned
%% on and clients optimistically read indirect storage in parallel with
%% their index read. Only if a value has changed do clients re-read from
%% extent storage. Due to atomic width limitations existing KV stores can
%% not gain this performance.

Prior systems have avoided locks due to the stark performance penalties of RDMA
atomics~\cite{design-guidelines}.  Figure~\ref{fig:rdma-benchmarks}(b)
illustrates this bottleneck on ConnectX-5 NICs when atomic operations are issued
on remote server memory: reads and writes scale almost linearly while atomics
reach a hard limit around 50~MOPS.  Using RDMA atomic operations to spin on
locks quickly induces these bottlenecks as the cap is even lower (3~MOPS as
shown in Figure~\ref{fig:rdma-benchmarks}(c)) for atomics that contend with each
other (i.e., access the same remote address).  Recent ConnectX series NICs sport
a small amount (256 KB) of on-NIC memory which avoids a PCIe round trip and
raises the cap on atomic operation
%scales linearly with all other RDMA operation
throughput~\cite{device-memory,sherman}. Figure~\ref{fig:rdma-benchmarks}(c)
illustrates the performance improvement for
%of on-NIC memory for CAS on a
single---i.e., contended---and independent addresses. If a system can arrange to
store its locks exclusively on NIC memory it can gain up to a 3x performance
improvement on contended workloads.

Others have explored ways to improve RDMA-based lock performance.  For
example, Citron implements a general purpose, range-based lock table
with fairness guarantees~\cite{citron}.  While the current design is
ill-suited for our use case (each request solves a knapsack problem,
maintains a tree structure, and can introduce false contention), we hope
to adapt its fair bakery algorithm to RCuckoo in future work.  At
present, RCuckoo focuses on raw, common-case performance, supporting
concurrent operations across disjoint ranges as described in the
following section.


% We considered Citron (a one-sided RDMA range lock manager)~\cite{citron}, as a potential lock
% manager for RCuckoo. It's desireable as an off-the-shelf solution because it's adaptable to RCuckoos
% lock locality. We deemed it's overheads too high for our use case as Citron required many additional
% RDMA messages to maintain it's tree structure, and due to it's need to solve a knapsack problem on
% each locking request, and because RCuckoo is not strictly range based, range locks would only
% increase contention. In the future we would like to explore alternative solutions to achieve
% fairness in RCuckoo.



% Recent ConnectX series NICs have a small amount of on-NIC memory on which atomic operations scale linearly with all other RDMA operations. Figure~\ref{fig:rdma-benchmarks}(c) illustrated the performance bottlenecks of CAS operations on device and main memory on a single address and on multiple addresses. If a data structure can organize its locks densely into NIC memory it can avoid the atomic bottleneck.
% Similarly, increasing scale and contention places greater pressure on
% the RDMA atomic operations necessary to serialize
% updates~\cite{design-guidelines,sherman}.
% Figure~\ref{fig:rdma-benchmarks}(c) shows the 100-Gbps ConnectX-5s in
% our testbed top out at around 50 MOPS per second when servicing atomic
% operations on remote memory, and less than 10 if those operations are
% contended.  Our testbed NICs, however—like those from several
% vendors—include a small amount (256 KB in our case) of on-NIC memory
% that can be addressed by remote clients using RDMA
% operations~\cite{device-memory}. Accesses to NIC memory avoid the need
% to cross the server’s PCIe bus, decreasing latency and increasing
% throughput.  CAS operations perform between 1.8--3.1$\times$ faster on
% NIC memory.  We design RCuckoo to perform atomic operations only on
% datstructures in NIC memory.


%% at these lower packet
%% rates the effects of contention are not as severe. 
%% RDMA atomic operations issued to main
%% memory have a bottleneck of around 50MOPS which can quickly become a
%% system bottleneck if clients are spinning on a lock. Recent connectX
%% series NICs have a small amount of on-NIC memory on which atomic
%% operations scale linearly with all other RDMA
%% operations. Figure~\ref{fig:rdma-benchmarks}(c) illustrated the
%% performance bottlenecks of CAS operations on device and main memory on
%% a single address and on multiple addresses. If a data structure can
%% organize it's locks densely into NIC memory it can avoid the atomic
%% bottleneck.


% \subsection{RDMA}
%%
% RDMA allows clients to directly access the memory of a remote server with one-sided operations like
% read, write and atomic update without the involvement of a remote CPU~\cite{infiniband-spec}.  To
% set the context for RCuckoo's design, we benchmark the baseline performance of our testbed hardware,
% illustrating the constraints and opportunities.

% Atomic operations such as compare-and-swap (CAS) are essential for implementing locks or
% opportunistic concurrency. Atomics are limited to 64-bit operations and bottleneck at lower rates
% than reads and writes because they block requests on data-dependent addresses while waiting on PCIe
% round trips~\cite{design-guidelines,sherman}.  Figure~\ref{fig:rdma-benchmarks

% While atomic operations are limited to 64 bits, read and write message sizes are bounded only by the
% link MTU.  Figure~\ref{fig:rdma-benchmarks}(b) shows that on our testbed NIC-to-NIC round-trip times
% are similar for all message sizes less than about 128 bytes, and messages must exceed 1~KB before
% the latency of a single large operation exceeds two RTTs of smaller ones.  We leverage this
% observation in RCuckoo by collapsing multiple small reads into a single larger one when appropriate.
% The optimal threshold trades off latency reduction against read amplification.


% Finally, we highlight that our testbed NICs, like those from several
% vendors, include a small amount (256~KB in our case) of on-NIC memory
% that can be addressed by remote clients using RDMA
% operations~\cite{device-memory}.  Accesses to NIC memory avoid the
% need to cross the server's PCIe bus, decreasing latency and increasing
% throughput.  The performance gain is particularly significant for
% atomic operations.  Figure~\ref{fig:rdma-benchmarks}(c) shows the
% maximum aggregate throughput of concurrent CAS operations targeting
% the same single (i.e., contended) address or distinct, independent
% addresses in both main server memory (shown in orange) and on-NIC
% device memory (blue).  CAS operations perform between 1.8--3.1$\times$
% faster on NIC memory.  RCuckoo's datastructures are designed so that
% all CAS operations are performed on NIC memory.


\subsection{Cuckoo and Hopscotch Hashing} 
\label{sec:cuckoo-back}

% A fully disaggregated KVS implements a concurrent hash table whose conflict resolution strategy is
% performed entirely by individual clients.  Cuckoo and hopscotch hashing are attractive algorithms in
% such settings as clients can determine a key's location without searching a remote index.
% Unfortunately, both algorithms have complex mechanisms for resolving hash collisions which involve
% iteratively searching and updating multiple index locations. In this section we describe both
% algorithms and their advantages and challenges in the disaggregated setting.


% \textbf{Cuckoo hashing} uses independent hash functions to compute a primary and secondary location
% for each key. Keys are always inserted into their primary location and evict an existing key to
% their secondary on collision which in turn can cause a chain of evictions (a cuckoo path) shown in
% Figure~\ref{fig:table-diagram}. Associative locations and BFS search are used to minimize path
% length~\cite{memc3,cuckoo-improvements} which is the primary factor in the cost of insertions.  Each
% path step incurs a random read to the index which requires an RDMA round trip. Proactively acquiring
% locks is not feasible as local caches would require the entire index, and quickly become stale as
% the index is updated.
% While insertions can have complex random memory accesses, reads can always be executed in a single
% round trip by reading the rows corresponding to both of a key's locations
% simultaneously~\cite{memc3,cuckoo-improvements,pilaf}.

% \textbf{Hopscotch hashing} is similar in that its reads also require only a single round
% trip~\cite{hopscotch,farm}. Its eviction policy differs from cuckoo hashing in that it evicts
% entries into a physically nearby location rather than a random one. Collisions are tracked with a
% per-entry bitmask which is updated on inserts. Critically, updating this bitmask requires locking an
% additional entry per update which multiples the number of updates per collision. Farm~\cite{farm}
% and Reno~\cite{reno} set hard limits on chain length but still require a server-side CPU to fix
% bitmasks when concurrent inserts execute.

% Both approaches have distinct pros and cons in how they manage collisions in the disaggregated
% setting. Cuckoo hashing has the potential to search random locations while searching for an open
% index, while hopscotch hashing has a bounded search but requires additional locking and state in the
% index. A system which avoided random search but does not require additional locking could
% potentially avoid the drawbacks of both approaches.

%%%%%%

Both cuckoo and hopscotch hashing have been highly successful as
indexes for RDMA
KVS~\cite{farm,memc3,reno,cuckoo-improvements,pilaf}. Both algorithms
enable clients to locally calculate a key's location (or neighborhood)
in the index without consulting the server. This fact enables a
critical division of labor for mixed read and write workloads. Reads
can be performed asynchronously via one-sided RDMA, while writes
require serialization---typically implemented using locks managed by
the CPU at the memory server using two-sided RDMA.  Both cuckoo and hopscotch hashing algorithms
have complex mechanisms for resolving hash collisions which make them
difficult to implement entirely with one-sided RDMA. This section
describes both algorithms and their associated challenges.

\textbf{Cuckoo hashing} uses independent hash functions to compute a primary and secondary location
for each key. Keys are always inserted into their primary location and evict an existing key to
their secondary on collision which in turn can cause a chain of evictions (a cuckoo path) shown in
Figure~\ref{fig:table-diagram}. Associative locations and BFS search are used to minimize path
length~\cite{memc3,cuckoo-improvements} which is the largest factor in the cost of insertions. 
%%
In a fully disaggregated implementation each path step incurs a read to a random index location
which requires an RDMA round trip (as the lack of locality makes client-side caching infeasible).  The lack of locality similarly frustrates attempts to proactively acquire locks.
%imilarly, proactively acquiring locks is not feasible as local caches would
%require the entire index and quickly become stale from concurrent updates to the table.
%%
That said, cuckoo hashing
%Although cuckoo hashing requires random memory accesses its hash has the excellent property that it
does not require any metadata to be maintained in the index, simplifying collision and error recovery.
%The complexity of its collision
%resolution is pushed entirely to its hash functions.

\textbf{Hopscotch hashing}, on the other hand, delivers a tunable
level of locality for insertions, but at the cost of maintaining index
metadata.  In hopscotch hashing, all insertions occur in a
neighborhood of their original hash
location~\cite{farm,hopscotch}. Evictions are performed by
``hopscotching'' entries through their neighborhood until an open
location is found.  Each entry has a bitmask which tracks its
collisions. When hopscotching these bitmasks must be updated which
requires additional locking.  Farm~\cite{farm} and Reno~\cite{reno}
bound this expense by setting hard limits on chain length but still
require a server-side CPU to fix bitmasks when concurrent inserts
execute.

Hopscotch hashing has the advantage of locality at the cost of additional metadata and locks, while
cuckoo hashing eschews metadata at the cost of random accesses. Our system RCuckoo combines the
advantages of both approaches by designing a cuckoo hash function with hopscotch-like locality
properties in the common case.
%%



% While insertions can
% have complex random memory accesses, reads can always be executed in a single round trip by reading
% the rows corresponding to both of a key's locations
% simultaneously~\cite{memc3,cuckoo-improvements,pilaf}.
