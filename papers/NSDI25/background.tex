
\section{Background}
\label{sec:background}

Existing disaggregated key/value stores build upon the extensive literature of
%are the highest performance technology for sharing disaggregated
RDMA-based key/value stores. In this section we overview that lineage
and discuss the algorithmic distinctions between the two. Our hash
table design draws on both cuckoo and hopscotch hashing. We
provide a brief overview of both approaches and related
literature.


% Key/value stores have long used RDMA to accelerate
% performance~\cite{farm,memc3,erpc,herd,faast,mica,pilaf,cell,storm}.  These systems strike a careful
% balance between directly accessing remote memory with one-sided RDMA requests and serializing
% mutating operations at a server-side CPU.  Cuckoo~\cite{cuckoo} and hopscotch~\cite{hopscotch}
% hashing are popular choices in this space because clients can calculate the location of keys without
% needing to consult the server and potentially perform lock-less reads with
% RDMA~\cite{farm,pilaf,cuckoo}.

\subsection{Disaggregated Key/Value Stores}

The challenge to adapting existing RDMA-based KVS designs to a fully
disaggreagated setting is the need to rely exclusively on one-sided
operations due to the lack of a server-side CPU.  Precursor RDMA KVS
systems achieve performance by striking a delicate balance between
efficient one-sided RDMA operations and CPU-serialized two-sided
operations~\cite{farm,herd,pilaf,cell,storm}, where the later are used
only judiciously, but are key to correctness.
%Disaggregated
%KVS must only use one-sided operations even when performing complex
%operations like resolving a collision while inserting.

In general, the performance of any KVS on a mixed read/write workload
hinges on its serialization performance.  Fast consistent writes with
one-sided RDMA are hard because RDMA atomic operations operate on
small (8-byte) regions of memory and the cost of round trips is
high. This constraint has lead to a divide in the design of
disaggregated KVS systems between those that support range queries over
ordered keys and those that do not. Systems with ordered keys use
locks to guard their complex updates, while those
with unordered keys employ lock-free optimistic approaches by
constraining updates to atomic 8-byte writes. These latter designs
typically use hash indexes with 8-byte entries that point to
uncontested extent regions where the corresponding values are stored.
Commonly the index entries use 48 of the 64 bits as a pointer and the
remaining 16 bits for a digest of the key and the size of the value.
This approach requires additional round trips to check for a key's
presence and limits the size of values that can be
stored~\cite{fusee,race}.

On the other hand, KVS systems that support ordered keys use
locks to guard the complex operations required to update B-Trees or
Radix Trees.  In exchange for expensive update operations, locks allow
%The advantage of locks is that
these systems to support inlined values and, thus, faster small-value
reads.  Performance is gated, however, by lock granularity and hold
times. At the time of writing all lock-based KVS systems assume that
clients are grouped into colocated servers which can locally
coordinate lock accesses and batch their writes~\cite{smart,sherman},
an assumption that optimistic
KVS systems do not make~\cite{ditto,fusee,clover,race}. We show that it is possible to
realize the performance gain of lock-guarded inlined operations
without requiring client co-location if locks are sufficiently fine
grained and hold times limited.
%critical sections are kept to 1-RTT and locks are as fine grained as possible.

\subsection{RDMA and Network Performance}

Historically, network bandwidth has limited KVS throughput more than
operation rate. For instance, while a 40-Gbps ConnectX-3 NIC can
process 75 million packets per second, line rate restricts it to
5~MOPS when reading 1-KB objects. As commodity
datacenter link rates increase from 100 Gbps to 400 Gbps and beyond,
however, latency and contention---rather than raw network
bandwidth---becomes the primary scalability bottleneck. To this end
our design choices reduce round trips at the cost of bandwidth
utilization.

Concretely, Figure~\ref{fig:rdma-benchmarks}(a) shows the tradeoff in
round trip time for RDMA reads of increasing size. Critically the
round trip time for a single 1-KB read is lower than the latency of
issuing two dependent reads of just a few bytes each.  This suggests
that performance can be gained if a single large packet can complete
the work of two dependent smaller
messages.
To evaluate this decision, our evaluation focuses on
small-key workloads whose performance is already gated by round-trip
access latency and contention on our 100-Gbps testbed.


Figure~\ref{fig:rdma-benchmarks}(b) shows the performance
overheads of using an extent based design on small key-value
pairs. Inlined values are read directly from the index while all other
values are read from indirect storage. Here client caching is turned
on and clients optimistically read indirect storage in parallel with
their index read. Only if a value has changed do clients re-read from
extent storage. Due to atomic width limitations existing KV stores can
not gain this performance.

Similarly, increasing scale and contention places greater pressure on
the RDMA atomic operations necessary to serialize
updates~\cite{design-guidelines,sherman}.
Figure~\ref{fig:rdma-benchmarks}(c) shows the 100-Gbps ConnectX-5s in
our testbed top out at around 50 MOPS per second when servicing atomic
operations on remote memory, and less than 10 if those operations are
contended.  Our testbed NICs, however—like those from several
vendors—include a small amount (256 KB in our case) of on-NIC memory
that can be addressed by remote clients using RDMA
operations~\cite{device-memory}. Accesses to NIC memory avoid the need
to cross the server’s PCIe bus, decreasing latency and increasing
throughput.  CAS operations perform between $1.8--3.1\times$ faster on
NIC memory.  We design RCuckoo to perform atomic operations only on
datstructures in NIC memory.


%% at these lower packet
%% rates the effects of contention are not as severe. 
%% RDMA atomic operations issued to main
%% memory have a bottleneck of around 50MOPS which can quickly become a
%% system bottleneck if clients are spinning on a lock. Recent connectX
%% series NICs have a small amount of on-NIC memory on which atomic
%% operations scale linearly with all other RDMA
%% operations. Figure~\ref{fig:rdma-benchmarks}(c) illustrated the
%% performance bottlenecks of CAS operations on device and main memory on
%% a single address and on multiple addresses. If a data structure can
%% organize it's locks densely into NIC memory it can avoid the atomic
%% bottleneck.


% \subsection{RDMA}
%%
% RDMA allows clients to directly access the memory of a remote server with one-sided operations like
% read, write and atomic update without the involvement of a remote CPU~\cite{infiniband-spec}.  To
% set the context for RCuckoo's design, we benchmark the baseline performance of our testbed hardware,
% illustrating the constraints and opportunities.

% Atomic operations such as compare-and-swap (CAS) are essential for implementing locks or
% opportunistic concurrency. Atomics are limited to 64-bit operations and bottleneck at lower rates
% than reads and writes because they block requests on data-dependent addresses while waiting on PCIe
% round trips~\cite{design-guidelines,sherman}.  Figure~\ref{fig:rdma-benchmarks

% While atomic operations are limited to 64 bits, read and write message sizes are bounded only by the
% link MTU.  Figure~\ref{fig:rdma-benchmarks}(b) shows that on our testbed NIC-to-NIC round-trip times
% are similar for all message sizes less than about 128 bytes, and messages must exceed 1~KB before
% the latency of a single large operation exceeds two RTTs of smaller ones.  We leverage this
% observation in RCuckoo by collapsing multiple small reads into a single larger one when appropriate.
% The optimal threshold trades off latency reduction against read amplificaiton.


% Finally, we highlight that our testbed NICs, like those from several
% vendors, include a small amount (256~KB in our case) of on-NIC memory
% that can be addressed by remote clients using RDMA
% operations~\cite{device-memory}.  Accesses to NIC memory avoid the
% need to cross the server's PCIe bus, decreasing latency and increasing
% throughput.  The performance gain is particularly significant for
% atomic operations.  Figure~\ref{fig:rdma-benchmarks}(c) shows the
% maximum aggregate throughput of concurrent CAS operations targeting
% the same single (i.e., contended) address or distinct, independent
% addresses in both main server memory (shown in orange) and on-NIC
% device memory (blue).  CAS operations perform between 1.8--3.1$\times$
% faster on NIC memory.  RCuckoo's datastructures are designed so that
% all CAS operations are performed on NIC memory.


\subsection{Concurrent hash tables} 
\label{sec:cuckoo-back}

A fully disaggregated KVS implements a concurrent hash table whose
conflict resolution strategy is performed entirely by individual
clients.
%~\cite{rolex,smart,fusee,race,sherman,ditto}.
Cuckoo and hopscotch hashing are attractive algorithms in such
settings as clients can determine a key's location without searching a
remote index. Unfortunately, both algorithms have complex mechanisms
for resolving hash collisions which involve iteratively searching and
updating multiple index locations. In this section we describe both
algorithms and their advantages and challenges in the disaggregated
setting.

% Like any hash table, the
% underlying hashing algorithm must have an approach to managing
% collisions. 

% Cuckoo and hopscotch hashing are particularly attractive
% in this context, because they both provide the property that the
% potential locations of an entry in the table, regardless of
% contention or collision, can be deterministically computed by clients
% based only upon the key itself. 

% Moreover, the set of locations is
% limited.  Hence, at least in theory, systems built around either
% cuckoo or hopscotch hashing hold the potential for single-round-trip
% reads.

%\todo{Make sure to describe how a cuckoo path works}

\textbf{Cuckoo hashing} uses independent hash functions to compute two
potential table locations for a key, a primary and a secondary, where
each location corresponds to an associative row of entries.  A key is
always inserted into its primary location.  If that row is full, an
existing key is evicted (or ``cuckooed'') to its secondary row to make
space. If the cuckooed entry's secondary row is also full, the process
iterates---by selecting yet another entry in the secondary row to
cuckoo---until an open location is found. The chain of evictions is
known as a~\textit{cuckoo path}.  Figure~\ref{fig:table-diagram} (our
table diagram) shows an example of a cuckoo path of length 1. In
practice when tables are nearly full paths can grow long and each step
in the cuckoo path generates reads to random regions of the index.
While insertions can be complex, reads can always be executed in a
single round trip by reading the rows corresponding to both of a key's
locations simultaneously~\cite{memc3,cuckoo-improvements,pilaf}.

\textbf{Hopscotch hashing} works in a similar fashion but provides a slightly different guarantee,
namely that keys will be located within a bounded neighborhood.  (While cuckoo hashing limits the
number of locations in which a key may be stored, it does not provide any locality guarantees
regarding those locations.) It does so by finding the physically closest empty entry to the desired
location and then, if that location is not within the neighborhood, iteratively moving other entries
out of the way to make room for the new key.  The hopscotch process is facilitated by maintaining a
per-entry bitmask of nearby collisions.  As with cuckoo hashing, clients can read in a single round
trip time by reading a key's entire neighborhood at once.


The insert operation is expensive for both approaches, and prior
systems have taken steps to mitigate its cost.  In associative hashes
like cuckoo hash tables, multiple entries can be chosen as eviction
candidates and breadth-first-search (BFS) has been shown to minimize
both cuckoo-path length and critical section
time~\cite{memc3,cuckoo-improvements}.
%While insertions require large
%critical sections to perform search and execute updates along the
%cuckoo path
Farm~\cite{farm} and Reno~\cite{reno}, two systems based on hopscotch
hashing, completely avoid executing long hopscotch chains due to their
execution time and complexity.  Moreover, under either approach, the
insert operation can fail despite vacant entries in the table---they
are just too far away to be reached by either the cuckoo path or
hopscotch's neighborhood-bounded linear probing.  The point at which
inserts being to fail, known as the \emph{maximum fill factor}, is a
function of the number of hash locations and row associativity in
cuckoo hashing and desired neighborhood size for hopscotch hashing.

RCuckoo uses cuckoo rather than hopscotch hashing due to locking
concerns.
%While both alog
%also limits a key's location to a bounded set of 
%bounded range of addresses and we considered using it as our core data
%structure. We believe that although hopscotch hashing can likely be
%disaggregated efficiently it is less straightforward than cuckoo
%hashing for the following reasons:
%%
First, each step of a cuckoo insert process requires one update---to
the entry being moved to its secondary location---rather than two.
When an entry is relocated in a hopscotch table, the collision bitmask
must also be updated.  (Reno~\cite{reno} uses one-sided atomics to
sloppily update the bitmask but requires a server-side CPU to fix the
bitmasks whenever concurrent inserts execute.)  Second, keys exist in
one of two locations in cuckoo hashing so updates and deletes require
locking only two rows, while hopscotch entries inhabit a range of
locations, so a conservative locking strategy must lock the entire
range.  Yet, RCuckoo takes inspiration from hopscotch neighborhoods and employs dependent hashing to increase the spatial
locality of key locations, enabling clients to use local
caches to speculatively compute cuckoo paths.

%Because each lock
%acquisition is expensive this increases the fundamental difficulty of
%disaggregation a hopscotch hash.



%%

%Cuckoo hashes use
%associative rows to improve their maximum fill factors.

%%now I want to explain why cuckoo hashing and disaggregation don't work well together.

%% Lockless $O(1)$ reads make Cuckoo hashing a desireable
%% candidate for a disaggregated index. However, long
%% unpredictable cuckoo paths and RDMA CAS limitations make
%% performing insertions without locks difficult in the
%% disaggregated setting. We designed an RDMA based cuckoo hash
%% to illustrate the difficulties of performing opportunistic
%% insertions. On inserts this system makes a sequence of reads
%% to calculate a valid cuckoo path and then itteratitivly
%% issues CAS operations to swap value along the path. If any
%% value on the path is concurrently modified by another client
%% the insertions will fail and must restart.
%% Figure~\ref{fig:cuckoo-problems}(a) shows how the failure
%% rate of insertions filling a table from 80-90\% full.


%% \subsection{Full disaggregation}

%% %%
%% Disaggregated key-value in contrast assume that a memory
%% server cannot provide serialization and orchestrate their
%% writes solely using
%% clients~\cite{rolex,fusee,clover,sherman,ford,race}. With
%% the exception of Sherman~\cite{sherman} these systems use
%% systems commit writes optimistically using 64 bit RDMA CAS. 
%% %%
%% Opportunistic writes have the advantage that updates are
%% atomically visible, no critical sections exist, and client
%% failures do not leave the table in an inconsistent state.
%% Unfortunately CAS based opportunistic writes perform poorly
%% under contention leading to high tail latencies
%% ~\cite{clover}. Additionally RDMA CAS does not scale
%% well~\cite{design-guidelines}(Figure~\ref{fig:rdma-benchmarks}(b)),
%% and their small word width and lack of multi-CAS support
%% constrain the size and types of updates they can perform.

%% CAS width in particular effects system design because
%% key-value pairs can rarely fit into 64 bits, indexes updated
%% with CAS must reference keys values indirectly with a
%% pointer. At minimum resolving a remote pointer requires an
%% additional round trip for every read~\cite{race,clover}.
%% %%
%% As we will show in the following section data structures
%% like cuckoo and hopscotch hashes are difficult implement with
%% optimistic CAS updates because they require multiple updates
%% to execute atomically.
