\section{Introduction}
\label{sec:intro}

Key-value stores remain a fundamental building block of modern
scale-out service architectures.  Recent trends in primary storage
technologies, however, are pushing operators to consider ever-more
disaggregated designs for their in-memory storage systms.  Concretely,
increases in CPU core counts coupled with stagnating DRAM density has
led to an increased interest in remote memory tiers, where some
portion of memory is connected to the server over a network.
Moreover, once deaggregated, memory can be pooled to achieve greater
cost and power efficiencies.  The key challenge facing fully
disaggregate memory pools, however, is managing concurrent updates in
the face of high access latencies---e.g., 20$\times$ in the case of
Ethernet-based RDMA.

While
cache-coherent technologies like CXL are beginning to become
available, RDMA remains the commodity option for large-scale in-memory
key-value stores.
%% made DRAM a scarce resource. In response data center architects have
%% proposed resource disaggregation to improve resource utilization,
%% scalability and efficiency as well as increase per-core access to
%% memory. Disaggregation calls for the partitioning of a servers
%% resources into distinct pools (compute, memory, storage,
%% accelerators), which are interconnected by a high speed network.  In a
%% fully disaggregated model distinct servers house resources such as
%% memory servers which contain only memory, and no colocated CPU for
%% processing.
% DRAM capacity is scaling at an ever decreasing rate. As such
% the demand in data centers for high efficiency memory
% systems grows every year to improve memory utilization and
% sharing. Memory disaggregation i.e separating compute and
% memory into large pools is a promising approach to increase
% a CPU's per core memory access beyond what is available in a
% single machine while simultaneously decreasing per machine
% memory stranding.
%
%% Full disaggregation is challenging as it requires new system
%% designs to cope with the additional network latency.  For
%% example accessing intra-rack remote DRAM has a 20x (50ns to
%% 1us) overhead in relation to local memory. Cores sharing
%% remote memory must suffer a proportionally high cost for
%% synchronization when sharing pools of memory.
%Many prior
%disaggregated systems have eschewed sharing for static
%partitioning to avoid the high overheads~\cite{
%reigons,blade-server,legoos}.
Many prior RDMA-based systems opt for
a partially disaggregated model, in which a CPU core is
colocated with remote memory to provide the needed
serialization~\cite{erpc,herd,pilaf,cell,clover}.
%%  ~\cite{clover}. These designs are conceptually
%% close to prior work on RDMA key value stores which gain
%% performance benefits from having some operations use
%% one-sided RDMA verbs\footnote{one-sided verbs are
%% disaggregation compliant as they do not require a CPU on the
%% receiver side during operation}
%% %% %
%% While highly performant these approaches do not conform to
%% fully disaggregated designs.
%
% to remote memory are approximately 20x that
% of local DRAM (50ns to 1us) which severely increases the
% cost of synchronizing shared data structures among many
% CPU's across multiple machines. Traditional and partially
% disaggregated key-value serialize updates with a CPU
% colocated with
% memory~\cite{memcached,herd,erpc,pilaf,cell,clover}(\todo{check
% sherman}). This approach does not meet the requirements of
% full disaggregation in which memory is a passive resource.
% In the fully disaggregated setting serialization is achieved
% entirely with a client based protocol. 
%
The few proposals that leverage one-sided RDMA atomic operations
(e.g., compare-and-swap) to implement fully disaggregated hash
tables~\cite{rolex,fusee,sherman, race} suffer from the limited scope
of such operations.  Specifically, RDMA atomic operations can only act
upon 64 contiguous bits of memory at a time, forcing these systems to design
multi-stage datastructures to support practical key and value sizes.

In general, these key-value stores implement a high-performance index
structure to localize contention, and then store the value separately,
requiring multiple operations and network round trips.  Early
systems~\cite{sherman} employed fine-grained lock tables, while more
recent designs extract higher performance in read-heavy workloads
through optimistic approaches to metadata management.  Unfortunately,
optimistic approaches can perform worse under high contention,
leading to an awkward trade space.


%% These approaches either use optimistic
%% concurrency, or assume that most clients are colocated and
%% can resolve their lock acquisitions in local memory.
%% Optimistic approaches using RDMA atomics are limited by the
%% width of the atomic instruction (64 bits for compare and
%% swap). 64 bits is often too small to encapsulate a full
%% key-value update. The result is that optimistic approaches
%% require more round trips to remote memory to complete their
%% operations. These approaches have dismissed lock based
%% approaches outright as the size of their critical sections
%% can bottleneck performance, and  the cost of acquiring and
%% releasing locks (at minimum two round trips) is often higher
%% than the optimistic approach (two or three round trips).

% %concurrent data structures round trips
% Concurrent data structure design for remote memory is hard.
% Access latency to remote memory is high so round trips per
% operation must be minimized to achieve efficiency.
% Serialization is particularly hard because there is no
% centralized serialization point to guard access to remote
% memory. RDMA NIC's provide atomic verbs such as
% compare-and-swap (CAS), but these are by no means a silver
% bullet.  Each atomic request takes a round trip from client
% to server to execute. In the best case lock/unlock requires
% two round trips, if multiple locks are required, or locks
% are contested the number of round trips increases.

% %cpu locking vs rdma
% In traditional key-value stores (Memcached~\cite{memcached})
% the CPU coordinates table access for read, write, and lock
% instructions. In contrast RDMA based Key-value
% stores~\cite{herd,erpc,pilaf} use a mixture of one-sided (no
% cpu) and two-sided (cpu involved) verbs to alleviate the CPU
% bottleneck. Reads are typically one sided to bypass the CPU
% bottleneck~\cite{pilaf,cell} while writes are typically two
% sided so memory-side CPU can orchestrate serialized
% operations (e.g. locking) with main memory access latency
% (50-100ns).  These small access times keep critical sections
% small for CPU based locking, and dramatically increase them
% for one-sided RDMA based locking schemes~\cite{clover,
% sherman}.

%rdma and cuckoo hashing
In this work we consider designing an index datastructure specifically
tailored for the constraints of current RDMA hardware.  Specicially,
we seek to improve the performance of a lock-based approach by
decreasing the number of RDMA operations required to acquire the locks
necessary to perform updates.  Our contribution is a concurrent hash
table that employs a novel form of Cuckoo hashing designed
specifically to perform well with one-sided RDMA operations. In the
common case our reads execute in a single round trip, and updates to
the table require only two round trips; insert operations have a median
of two round trips.

Our design employs multiple distinct techniques to decrease the expected number of round trips required per operation.  The first is a
\emph{dependent} hashing algorithm. Cuckoo hashes use two (or more)
hash locations per key to resolve collisions. Dependent
hashing increases the likelihood that a key's hash locations
will be spatially near one another; if locations are close enough, a client can read them both with a single RDMA operation.  Of course, the increased locality
leads to a higher degree of collisions, impacting the maximum number of elements which can be inserted
into the table (i.e., fill factor).  We explore the trade space to find the best balance  between
performance and fill factor. Using this locality we design a
dense lock table which can fit into RDMA NIC device memory
as it provides up to 3$\times$ higher performance for contested
atomic access. Our locking protocol acquires and releases
locks in bulk using the fewest round trips possible. We use
Masked Compare and Swap to remove unessisary contention from
out lock table.

Using locks rather than CAS to modify our index allows our
index to support variable sized entries, and allows us to
inline keys and values directly into the index. Inlining
entries significantly reduces operation latency. We highly
optimize inserts into our hash table. Locality between
key-pairs improves insert locality enabling RCuckoo to both
find valid cuckoo paths efficiently and aquire all locks for
insertion in few round trips. We achieve this with a mixture
of smart caching, searching, and a two stage cuckoo path
finding algorithm. Finally locality improves reads by using
single larger reads when key pairs are close, rather than
multiple smaller reads.

% Inserting into a cuckoo hash table normally requires
% acquiring many locks randomly throughout the table,
% dependent hashing clusters lock acquisitions enabling many
% locks to be acquired in a single atomic request. Second, our
% algorithm uses speculative lock acquisition. Locks are
% acquired in bulk based on a local cache of the remote index.
% Once locks are held a second search on synchronized locked
% data is performed to ensure a correct insertion. Third we
% use locality to improve both read and search speeds. We find
% that using a single large read over two smaller reads
% improves throughput when key locations are known to be
% physically close.

% \sg{Sherman uses masked compare and swap as well as device
% memory, we might use it better but it's hard to claim a
% contribution here.}

We develop a prototype of our algorithm in C++ and evaluate
it against three recent RDMA based key value stores. And
provide microbenchmarks to analyze each of our design
decisions. Our evaluation shows that we achive better
performance than all prior approaches. On YCSB benchmarks we
We gain between 1.4x to 10x on write heavy workloads and
1.3x to 2.2x on read heavy workloads. 
