# Introduction

Dangerous times. It seems that as I scale out the hash table any configuration of suffix and bucket eventually crashes. I had an idea to try and mitigate this. If in the static case where the bound on each suffix is constant we run in to "knots" point in the hashing where we have no option but to stop, perhaps putting a few items in the hash which can cause the hash to jump out of the region and into another one. 

The idea is to put the suffix size on an exponential distribution with a small
mean, but a very long tail. The idea is that when we don't have a very full
table we should at least be able to search for a location that will "eject" us
to another part of the table where we could actually find a solution.

My thoughts on the implementation are that when we have an item with a really be
span (i.e the exponential one) we just issue two reads to find both location
exactly as RACE does. However in the common case we can get away with issuing a
single read

My code at the moment counts the number of zeros generated by the first hash function and uses that to determine how big the suffix of the value is.

```python
def secondary_bounded_location_exp(key, table_size, suffix_size):
    primary = primary_location(key, table_size)
    ps = str(bin(primary))[2:]
    zeros = (len(ps) - len(ps.rstrip('0')) + 1) * suffix_size
    secondary = (int(h2(key),16)) % (2**zeros)
    return (primary + secondary) % table_size
```

![memory_vs_fill](001_exp_hash_1_4.png)
